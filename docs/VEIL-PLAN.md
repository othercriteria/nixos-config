# Veil Cluster: Plan and Progress

This document is the working plan and checklist to introduce a new
headless Kubernetes cluster named "veil" on a set of hosts named
`meteor-1`, `meteor-2`, and `meteor-3`. It also tracks a light
refactor to place configuration at the appropriate level while keeping
`skaia` fully functional.

Keep lines under 80 columns; use checkboxes to track progress.

## Decisions (confirmed)

- HA control plane: 3x k3s servers (embedded etcd)
- Observability: in-cluster (no host-level Prometheus for meteors)
- Email alerts: not used for meteors
- IP addressing: static via DHCP reservations on router
- Storage: k3s local-path-provisioner (no ZFS on meteors)
- Secrets: git-secret under `secrets/`
- Host networking stack: systemd-networkd (default; can switch if needed)

## Network and addressing

- Router: Archer C2300 v2.0, with DHCP managing 192.168.0.100-192.168.0.219
- Switch: unmanaged 2.5 Gbps; invisible at L3 (no config required)
- Meteor nodes: reserved IPs via DHCP, one NIC each, DHCP client enabled
- MetalLB: allocate a dedicated, non-DHCP range on the LAN
  - Proposal: adjust router DHCP pool to end at `.219` and reserve
    `192.168.0.220-239` for MetalLB services
  - Alternative: choose any gap outside active DHCP pool (must avoid collisions)

Open item to finalize:

- [x] Confirm MetalLB address pool (192.168.0.220-239)

## Repository structure changes (safe, incremental)

Objective: introduce a dedicated headless server baseline without
impacting `skaia`.

- [x] Create `hosts/server-common/default.nix` (headless server baseline)
  - SSH, fail2ban, zsh, `dlk` user, PKI rootCA, essential tools
  - No GUI modules (`greetd`, `fonts`, `printing`), no ProtonVPN by default
  - Prefer systemd-networkd; NetworkManager not enabled by default
- [ ] Update `hosts/skaia/default.nix` to remain functionally unchanged
  - Keep importing `../common` for now
- [ ] (Optional next step) Extract a `modules/base-common.nix` from
  `hosts/common/default.nix` and create:
  - `hosts/desktop-common/default.nix` = base-common + desktop extras
  - `hosts/server-common/default.nix` = base-common + server extras
  - Then switch `skaia` to import `../desktop-common` (no behavior change)

Rationale: avoids churn to `skaia` during initial veil rollout, while
creating a clean server baseline for meteors.

## New hosts: meteor family

- [x] Add `hosts/meteor-1/` and `hosts/meteor-2/` and `hosts/meteor-3/`
  - `default.nix` imports `../server-common`, `./k3s`, `./firewall.nix`
  - `hardware-configuration.nix` (generated by installer)
  - `firewall.nix` for k8s/etcd/VXLAN ports
  - `k3s/default.nix` config for role and flags
  - `k3s/join-token.nix` to provide token via git-secret
- [x] Add `meteor-*` systems to `flake.nix`

Hostnames and addressing (example; to be finalized):

- `meteor-1` → 192.168.0.121 (server, `--cluster-init`)
- `meteor-2` → 192.168.0.122 (server)
- `meteor-3` → 192.168.0.123 (server)

## k3s cluster design (veil)

- All three meteors run `services.k3s.role = "server"`
- Cluster name: `--cluster-name veil`
- First server only: `--cluster-init`
- Join others with `--server https://<meteor-1-ip>:6443` and token
- Disable Traefik to use ingress-nginx
- Keep default local-path storage (do not disable `local-storage`)
- Expose control-plane metrics like `skaia`
- Version: stable release channel

Proposed k3s flags (shape; specifics handled in Nix):

- `--cluster-name veil`
- `--disable traefik`
- `--write-kubeconfig-mode 0644`
- `--kubelet-arg=authentication-token-webhook=true`
- `--kubelet-arg=authorization-mode=Webhook`
- `--kube-controller-manager-arg=bind-address=0.0.0.0`
- `--kube-proxy-arg=metrics-bind-address=0.0.0.0`
- `--kube-scheduler-arg=bind-address=0.0.0.0`

Token handling:

- Path: `/etc/nixos/secrets/veil-k3s-token`
- Managed via git-secret as `secrets/veil-k3s-token`

## Ingress and LoadBalancer

- [ ] Install MetalLB in L2 mode with chosen address pool
- [ ] Install ingress-nginx via Helm

Notes:

- Ensure MetalLB addresses are outside the router DHCP pool
- Unmanaged switch is fine for L2 advertisements

## DNS strategy for home network

Goal: predictable names for services exposed via MetalLB.

Options and recommendations:

- mDNS (`*.local`): works via multicast on the same L2 segment; good for
  hostnames, not ideal for Services with VIPs.
  - To use: enable mDNS on hosts (either Avahi or systemd-resolved):
    - Avahi approach:
      - `services.avahi.enable = true;`
      - `services.avahi.nssmdns = true;`
    - systemd-resolved approach:
      - `services.resolved.enable = true;`
      - `services.resolved.extraConfig = "MulticastDNS=yes";`
  - Note: `.local` is reserved for mDNS and should not be used for normal DNS
    A records.
- Router DNS with `home.arpa` (recommended):
  - Use a private zone like `veil.home.arpa` and add static A records on the
    router for ingress VIPs from MetalLB.
  - Example: `grafana.veil.home.arpa` → 192.168.0.220
  - Benefit: normal unicast DNS; no dependency on mDNS behavior.
- Quick testing without router DNS edits:
  - Use `sslip.io` or `nip.io` to map hostnames to an IP automatically.
  - Example host: `app.192-168-0-220.sslip.io` resolves to 192.168.0.220.

Plan:

- Prefer `veil.home.arpa` with static A records for user-facing services.
- Use `sslip.io` during bootstrap until DNS entries are added.
- See `docs/residence-1/ADDRESSING.md` for LAN addressing and DNS notes.

## Storage

- Use k3s local-path-provisioner for now (default, simple)
- Future options (out of scope for initial rollout):
  - Longhorn (recommended for modest clusters and simplicity)
  - Ceph (powerful, but heavier to operate on small hosts)

## Observability (in-cluster)

- [ ] Deploy `kube-prometheus-stack` via Helm
  - Prometheus Operator, Prometheus, Alertmanager, Grafana
  - Configure sensible retention for modest disk sizes
- [ ] (Optional) Deploy Loki + Promtail in-cluster for logs
- No SMTP/email alerts for meteors; dashboards and alerts in Grafana

## Installation guidance for meteors

Recommended flow for each meteor:

- Installation
  - Use the graphical installer with default options (UEFI, ext4), no ZFS.
  - Swap: skip disk swap; we will enable `zramSwap` in server config.
  - Create a temporary admin user (can be `dlk`); SSH enabled.
- First boot
  - Clone the repo to your workspace (consistent with `skaia`):
    - `mkdir -p ~/workspace && cd ~/workspace`
    - `git clone git@github.com:othercriteria/nixos-config.git`
    - `cd nixos-config`
  - Dev shell with flakes enabled:
    - `nix develop --extra-experimental-features nix-command \
      --extra-experimental-features flakes`
  - (Optional) Import GPG keys to reveal secrets:
    - Ensure `programs.gnupg.agent` is enabled (mirrored from `skaia`)
    - Import key and ownertrust; then `make reveal-secrets`
  - Place required secrets if skipping git-secret:
    - `sudo mkdir -p /etc/nixos/secrets`
    - `sudo install -m600 -o root -g root secrets/veil-k3s-token \
      /etc/nixos/secrets/veil-k3s-token`
  - Apply host using Makefile sugar:
    - `make apply-host HOST=meteor-1`
- Post-apply
  - Verify `k3s` is running on `meteor-1` (server, `--cluster-init`).
  - Repeat for `meteor-2` and `meteor-3` (server role joined via token).

Notes:

- GPG: On `skaia`, ensure pinentry works; we mirror the agent on meteors.
- Bootloader: `server-common` enables systemd-boot for UEFI by default.
- stateVersion: set to `24.11` in `server-common` (adjust if your initial
  install differs).
- Using `zramSwap` avoids disk wear and is sufficient for these nodes.
- If you prefer disk swap, allocate 4–8 GiB; not strictly required.

## Firewall policy (per node)

Server nodes (all meteors):

- Allow from cluster/admin nets to:
  - TCP 6443 (k8s API)
  - TCP 2379-2380 (etcd) between servers only
  - UDP 8472 (flannel VXLAN)
  - TCP 10250 (kubelet; for scraping)
- Node exporter ports only if scraped by external Prometheus (not used here)
- SSH limited to admin network(s)

## Cold start annotations and docs

Add explicit COLD START notes in code and summarize here:

- [ ] k3s HA boot order and join token
  - `# COLD START: Initialize meteor-1 with --cluster-init, then join others`
- [ ] Create and manage `veil-k3s-token` via git-secret
  - `# COLD START: Ensure /etc/nixos/secrets/veil-k3s-token exists`
- [ ] MetalLB IP pool carved out of LAN DHCP
  - `# COLD START: Reserve MetalLB IP range on router`
- [ ] (If any host-level observability later) ZFS dataset steps (not used now)

Update `docs/COLD-START.md` with step-by-step instructions when the
above are implemented.

## Bootstrap checklist

Preparation

- [x] Adjust router DHCP pool to avoid MetalLB range
- [x] Confirm MetalLB pool (192.168.0.220-239) and reserve it
- [ ] Create DHCP reservations for `meteor-1..3` (document IP↔MAC mapping)
- [ ] Create k3s join token secret:
  - `echo "..." > secrets/veil-k3s-token`
  - `git secret add secrets/veil-k3s-token`
  - `git secret hide`

Bring-up

- [x] Add `hosts/server-common` and `hosts/meteor-*` skeleton
- [x] Add `meteor-*` to `flake.nix`
- [ ] Build and boot `meteor-1` (server, `--cluster-init`)
- [ ] Verify API health: `kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get nodes`
- [ ] Build and boot `meteor-2` and `meteor-3` (server role)
- [ ] Verify etcd cluster healthy and all nodes Ready

Cluster services

- [ ] Install MetalLB and apply address pool config
- [ ] Install ingress-nginx
- [ ] Install kube-prometheus-stack
- [ ] (Optional) Install Loki + Promtail

Post-setup

- [ ] Create an admin kubeconfig for `dlk` with a `veil` context
- [ ] Add baseline dashboards and alerting rules in Grafana
- [ ] Document operational runbooks (backups, upgrades)

## Risks and mitigations

- HA etcd requires time sync; ensure NTP is enabled (systemd-timesyncd)
- MetalLB must not overlap with DHCP pool; finalize and reserve the range
- Local-path PVs are node-local; some stateful workloads may need Longhorn later
- Keep all three nodes on same L2 domain for flannel VXLAN and MetalLB L2

## Open questions

- Add TODO to migrate `skaia` to systemd-networkd later?
- [Planned] Move LAN DNS to `skaia` to host `veil.home.arpa` (interim: `sslip.io`)
- Network name documented: `residence-1` (see `docs/residence-1/ADDRESSING.md`)

## Notes

- A few lines of boilerplate per meteor host are acceptable
- We will keep `skaia` fully intact during initial rollout; optional
  refactors come after meteors are stable
- We can wire configs in parallel with `meteor-1` installation until
  `hardware-configuration.nix` is available
- Network/site name: `residence-1` (see `docs/residence-1/ADDRESSING.md`)
